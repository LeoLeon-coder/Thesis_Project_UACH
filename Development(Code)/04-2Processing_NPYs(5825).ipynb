{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "316b69e7",
   "metadata": {},
   "source": [
    "# Cell 1 — Load memmaps + trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cde52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled X_train: (52551, 3, 32, 201) float16\n",
      "Unlabeled X_unl: (687350, 3, 32, 201) float16\n",
      "Loaded model: C:\\Users\\leona\\Documents\\Thesis_Project_UACH\\Temp\\Dataset\\features_mfcc_labeled\\cnn_mfcc_best.keras\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths\n",
    "LABELED_DIR = Path(r\"C:\\Users\\leona\\Documents\\Thesis_Project_UACH\\Temp\\Dataset\\features_mfcc_labeled\")\n",
    "UNLABELED_DIR = Path(r\"C:\\Users\\leona\\Documents\\Thesis_Project_UACH\\Temp\\Dataset\\features_mfcc_unlabeled\")\n",
    "\n",
    "# Labeled (memmap)\n",
    "X_train = np.load(LABELED_DIR / \"X_train.npy\", mmap_mode=\"r\")\n",
    "y_train = np.load(LABELED_DIR / \"y_train.npy\")\n",
    "X_val   = np.load(LABELED_DIR / \"X_val.npy\",   mmap_mode=\"r\")\n",
    "y_val   = np.load(LABELED_DIR / \"y_val.npy\")\n",
    "X_test  = np.load(LABELED_DIR / \"X_test.npy\",  mmap_mode=\"r\")\n",
    "y_test  = np.load(LABELED_DIR / \"y_test.npy\")\n",
    "\n",
    "# Unlabeled features (memmap) - you must have this file\n",
    "X_unl = np.load(UNLABELED_DIR / \"X_unlabeled.npy\", mmap_mode=\"r\")\n",
    "\n",
    "print(\"Labeled X_train:\", X_train.shape, X_train.dtype)\n",
    "print(\"Unlabeled X_unl:\", X_unl.shape, X_unl.dtype)\n",
    "\n",
    "# Load your best model checkpoint\n",
    "MODEL_PATH = LABELED_DIR / \"cnn_mfcc_best.keras\"   # or wherever you saved it\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "print(\"Loaded model:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780299f6",
   "metadata": {},
   "source": [
    "# Cell 2 — Build a prediction dataset for unlabeled (memmap-safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c735f54e",
   "metadata": {},
   "source": [
    "This streams batches from ```X_unl``` and never loads it all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7ed8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "BATCH_PRED = 256  # increase if you have GPU VRAM\n",
    "\n",
    "unl_ds = tf.data.Dataset.from_tensor_slices(X_unl)\n",
    "unl_ds = unl_ds.batch(BATCH_PRED).map(lambda x: tf.cast(x, tf.float32), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "unl_ds = unl_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebb7a5",
   "metadata": {},
   "source": [
    "# Cell 3 — Generate pseudo-labels with confidence filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c453b2ee",
   "metadata": {},
   "source": [
    "This creates small arrays of ```selected_indices```, ```pseudo_labels```, and ```pseudo_confidence```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcbc733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "THRESH = 0.95  # confidence threshold (try 0.90–0.98)\n",
    "\n",
    "probs = model.predict(unl_ds, verbose=1)  # shape: (N_unl, 4)\n",
    "pseudo_y = probs.argmax(axis=1).astype(np.int64)\n",
    "conf = probs.max(axis=1).astype(np.float32)\n",
    "\n",
    "selected_mask = conf >= THRESH\n",
    "selected_idx = np.where(selected_mask)[0].astype(np.int64)\n",
    "selected_y = pseudo_y[selected_mask]\n",
    "selected_conf = conf[selected_mask]\n",
    "\n",
    "print(\"Unlabeled total:\", len(probs))\n",
    "print(\"Selected pseudo-labeled:\", len(selected_idx))\n",
    "print(\"Selection rate: {:.2f}%\".format(100.0 * len(selected_idx) / max(1, len(probs))))\n",
    "\n",
    "# Class distribution among selected (useful to detect collapse into class 3)\n",
    "u, c = np.unique(selected_y, return_counts=True)\n",
    "print(\"Pseudo-label class counts:\", dict(zip(u, c)))\n",
    "print(\"Confidence stats: min/mean/max =\", float(selected_conf.min()), float(selected_conf.mean()), float(selected_conf.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36b3434",
   "metadata": {},
   "source": [
    "prevent pseudo labels from being 95% class “3”. If your selected set is overwhelmingly class 3, cap per class to keep diversity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a55a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PER_CLASS = 20000  # cap per pseudo class (adjust)\n",
    "\n",
    "kept = []\n",
    "for k in range(4):\n",
    "    idx_k = selected_idx[selected_y == k]\n",
    "    if len(idx_k) > MAX_PER_CLASS:\n",
    "        # keep highest-confidence ones for that class\n",
    "        conf_k = selected_conf[selected_y == k]\n",
    "        top = np.argsort(-conf_k)[:MAX_PER_CLASS]\n",
    "        idx_k = idx_k[top]\n",
    "    kept.append(idx_k)\n",
    "\n",
    "selected_idx = np.concatenate(kept).astype(np.int64)\n",
    "# Recompute selected_y/conf for the capped indices\n",
    "selected_y = pseudo_y[selected_idx]\n",
    "selected_conf = conf[selected_idx]\n",
    "\n",
    "print(\"After per-class cap -> Selected:\", len(selected_idx))\n",
    "u, c = np.unique(selected_y, return_counts=True)\n",
    "print(\"Capped pseudo-label counts:\", dict(zip(u, c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbff693",
   "metadata": {},
   "source": [
    "# Cell 4 — Build ```tf.data``` datasets WITHOUT concatenating big arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c95940",
   "metadata": {},
   "source": [
    "This is the key part. We will not do ```np.concatenate([X_train, X_pseudo])```.\n",
    "Instead we build two datasets and mix them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13519be7",
   "metadata": {},
   "source": [
    "## 4A) Labeled dataset (memmap streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df19e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_TRAIN = 64\n",
    "\n",
    "def make_labeled_ds(X, y, training=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if training:\n",
    "        ds = ds.shuffle(20000, seed=SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(BATCH_TRAIN)\n",
    "    ds = ds.map(lambda a,b: (tf.cast(a, tf.float32), tf.cast(b, tf.int64)),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds_labeled = make_labeled_ds(X_train, y_train, training=True)\n",
    "val_ds = make_labeled_ds(X_val, y_val, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e48ef",
   "metadata": {},
   "source": [
    "## 4B) Pseudo-labeled dataset via indices (true memmap-safe, no copies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e5cdb4",
   "metadata": {},
   "source": [
    "We store only ```selected_idx``` and fetch ```X_unl[idx]``` on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d5186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make globals accessible inside py_function\n",
    "X_unl_global = X_unl\n",
    "selected_y_global = selected_y  # aligned with selected_idx positions\n",
    "\n",
    "def _get_pseudo_sample(i):\n",
    "    # i is an index into selected_idx array (0..len(selected_idx)-1)\n",
    "    i = int(i)\n",
    "    real_idx = int(selected_idx[i])\n",
    "    x = np.array(X_unl_global[real_idx], dtype=np.float32)  # (3,32,201)\n",
    "    y = np.int64(selected_y_global[i])\n",
    "    return x, y\n",
    "\n",
    "def tf_get_pseudo_sample(i):\n",
    "    x, y = tf.py_function(_get_pseudo_sample, [i], [tf.float32, tf.int64])\n",
    "    x.set_shape((3, 32, 201))\n",
    "    y.set_shape(())\n",
    "    return x, y\n",
    "\n",
    "pseudo_ds = tf.data.Dataset.from_tensor_slices(np.arange(len(selected_idx), dtype=np.int64))\n",
    "pseudo_ds = pseudo_ds.shuffle(min(len(selected_idx), 20000), seed=SEED, reshuffle_each_iteration=True)\n",
    "pseudo_ds = pseudo_ds.map(tf_get_pseudo_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "pseudo_ds = pseudo_ds.batch(BATCH_TRAIN).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Pseudo dataset batches ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dae4bd",
   "metadata": {},
   "source": [
    "## 4C) Mix labeled + pseudo-labeled (no huge arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef381d4",
   "metadata": {},
   "source": [
    "Use ```sample_from_datasets``` so each batch is drawn from one source or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df23db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting: start conservative (more real labels than pseudo)\n",
    "W_LABELED = 0.7\n",
    "W_PSEUDO  = 0.3\n",
    "\n",
    "train_ds_mixed = tf.data.Dataset.sample_from_datasets(\n",
    "    [train_ds_labeled, pseudo_ds],\n",
    "    weights=[W_LABELED, W_PSEUDO],\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e7821",
   "metadata": {},
   "source": [
    "# Cell 5 — Fine-tune (continue training) with EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba79555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep compile same; maybe lower LR for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),  # smaller LR for fine-tune\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\", mode=\"max\",\n",
    "        patience=10, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_accuracy\", mode=\"max\",\n",
    "        patience=3, factor=0.5\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds_mixed,\n",
    "    validation_data=val_ds,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0636e55",
   "metadata": {},
   "source": [
    "# Cell 6 — Evaluate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c13438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "test_ds = make_labeled_ds(X_test, y_test, training=False)\n",
    "probs_test = model.predict(test_ds, verbose=0)\n",
    "y_pred = np.argmax(probs_test, axis=1)\n",
    "\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
