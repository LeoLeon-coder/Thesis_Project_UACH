{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48991d77",
   "metadata": {},
   "source": [
    "## Celda 1 — import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd13f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4007882",
   "metadata": {},
   "source": [
    "## Celda 2 — Configuración (rutas + columnas), Cargar CSV, filtrar etiquetados, mapear labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6188dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetados: 1275 | faltantes sin path: 0\n",
      "Conteo por clase:\n",
      " y\n",
      "0    179\n",
      "1    158\n",
      "2    259\n",
      "3    679\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(r\"C:\\Users\\leona\\Documents\\Thesis_Project_UACH\\Temp\\Dataset\\BeesAnna\\sound_files\")\n",
    "CSV_PATH = Path(r\"C:\\Users\\leona\\Documents\\Thesis_Project_UACH\\Temp\\Dataset\\BeesAnna\\all_data_updated.csv\")\n",
    "\n",
    "ID_COL = \"file name\"            # <-- columna de nombres\n",
    "TARGET_COL = \"queen status\"     # <-- la etiqueta que se utilizará\n",
    "VALID_CLASSES = {0, 1, 2, 3}\n",
    "\n",
    "# 1) Construir diccionario: nombre_archivo.wav -> ruta_completa\n",
    "wav_paths = sorted(list(DATA_DIR.rglob(\"*.wav\"))) + sorted(list(DATA_DIR.rglob(\"*.WAV\")))\n",
    "name_to_path = {p.name: str(p) for p in wav_paths}  # str() para que sea serializable y fácil\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "assert ID_COL in df.columns, f\"Falta ID_COL={ID_COL} en el CSV\"\n",
    "assert TARGET_COL in df.columns, f\"Falta TARGET_COL={TARGET_COL} en el CSV\"\n",
    "\n",
    "# 2) Normaliza nombres (solo basename)\n",
    "df[\"_basename\"] = df[ID_COL].astype(str).map(lambda s: Path(s).name)\n",
    "\n",
    "# 3) Convertir queen status a numérico y filtrar clases válidas\n",
    "df[\"_y\"] = pd.to_numeric(df[TARGET_COL], errors=\"coerce\")\n",
    "labeled = df[df[\"_y\"].isin([0,1,2,3])].copy()\n",
    "labeled[\"y\"] = labeled[\"_y\"].astype(np.int64)\n",
    "\n",
    "# 4) Crear columna \"path\" (ruta completa al wav)\n",
    "labeled[\"path\"] = labeled[\"_basename\"].map(name_to_path)\n",
    "\n",
    "\n",
    "\n",
    "# 5) Verificar que no falte ninguno\n",
    "missing = labeled[\"path\"].isna().sum()\n",
    "print(\"Etiquetados:\", len(labeled), \"| faltantes sin path:\", int(missing))\n",
    "assert missing == 0, \"Hay archivos etiquetados que no se encontraron en la carpeta.\"\n",
    "print(\"Conteo por clase:\\n\", labeled[\"y\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd41cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio / MFCC params (igual que la prueba \"TestMFCC\")\n",
    "SR = 16000\n",
    "TRIM_DB = 30\n",
    "SEG_SEC = 2.0\n",
    "HOP_SEC = 1.0\n",
    "\n",
    "N_MFCC = 32\n",
    "N_FFT  = int(0.025 * SR)\n",
    "HOP_LEN= int(0.010 * SR)\n",
    "FMIN, FMAX = 20, SR//2\n",
    "\n",
    "ADD_DELTAS = True\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE  = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f7f01db",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path(r\"C:\\Users\\leona\\Documents\\Thesis_Project_UACH\\Temp\\Dataset\\features_mfcc_labeled\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064996f",
   "metadata": {},
   "source": [
    "## Celda 3 — Funciones: limpiar, segmentar, MFCC (+Δ +ΔΔ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdfaea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_normalize(x, eps=1e-9):\n",
    "    return x / (np.max(np.abs(x)) + eps)\n",
    "\n",
    "def load_and_clean(path):\n",
    "    x, _ = librosa.load(str(path), sr=SR, mono=True)\n",
    "    x, _ = librosa.effects.trim(x, top_db=TRIM_DB)\n",
    "    x = peak_normalize(x)\n",
    "    return x\n",
    "\n",
    "def segment_signal(x, sr, seg_sec, hop_sec):\n",
    "    seg_len = int(seg_sec * sr)\n",
    "    hop_len = int(hop_sec * sr)\n",
    "    if len(x) < seg_len:\n",
    "        x = np.pad(x, (0, seg_len - len(x)), mode=\"reflect\")\n",
    "    segments = []\n",
    "    for start in range(0, max(1, len(x)-seg_len+1), hop_len):\n",
    "        seg = x[start:start+seg_len]\n",
    "        if len(seg) < seg_len:\n",
    "            seg = np.pad(seg, (0, seg_len - len(seg)), mode=\"reflect\")\n",
    "        segments.append(seg)\n",
    "    return segments\n",
    "\n",
    "def mfcc_features(seg):\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=seg, sr=SR, n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP_LEN,\n",
    "        fmin=FMIN, fmax=FMAX\n",
    "    )\n",
    "    if ADD_DELTAS:\n",
    "        d1 = librosa.feature.delta(mfcc)\n",
    "        d2 = librosa.feature.delta(mfcc, order=2)\n",
    "        feat = np.stack([mfcc, d1, d2], axis=0)  # (3, n_mfcc, T)\n",
    "    else:\n",
    "        feat = mfcc[np.newaxis, :, :]            # (1, n_mfcc, T)\n",
    "    return feat.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c0039",
   "metadata": {},
   "source": [
    "## Celda 4 — Split estratificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b204df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test: 1083 / 33 / 159\n"
     ]
    }
   ],
   "source": [
    "paths = labeled[\"path\"].values\n",
    "ys    = labeled[\"y\"].values\n",
    "\n",
    "# paths, ys ya definidos (por archivo, no por segmento)\n",
    "paths_train, paths_tmp, y_train, y_tmp = train_test_split(\n",
    "    paths, ys, test_size=(TEST_SIZE + VAL_SIZE), random_state=RANDOM_SEED, stratify=ys\n",
    ")\n",
    "\n",
    "# aquí tmp es (val+test); lo partimos a la mitad si VAL_SIZE==TEST_SIZE\n",
    "rel_test = TEST_SIZE / (TEST_SIZE + VAL_SIZE)\n",
    "\n",
    "paths_val, paths_test, y_val, y_test = train_test_split(\n",
    "    paths_tmp, y_tmp, test_size=rel_test, random_state=RANDOM_SEED, stratify=y_tmp\n",
    ")\n",
    "\n",
    "print(\"Train/Val/Test:\", len(paths_train), \"/\", len(paths_val), \"/\", len(paths_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3766e049",
   "metadata": {},
   "source": [
    "## Celda 5 — Extraer MFCC por segmento y “expandir” etiquetas a segmentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b72aa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo TRAIN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leona\\.conda\\envs\\envBees\\lib\\site-packages\\librosa\\core\\intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo VAL...\n",
      "Extrayendo TEST...\n",
      "Shapes:\n",
      "X_train: (63803, 3, 32, 201) y: (63803,)\n",
      "X_val: (1944, 3, 32, 201) y: (1944,)\n",
      "X_test: (9365, 3, 32, 201) y: (9365,)\n"
     ]
    }
   ],
   "source": [
    "def process_labeled_file_list(file_list, label_list):\n",
    "    feats = []\n",
    "    labs = []\n",
    "    file_index = []  # trazabilidad: (archivo, segmento)\n",
    "    for p, y in zip(file_list, label_list):\n",
    "        x = load_and_clean(p)\n",
    "        segs = segment_signal(x, SR, SEG_SEC, HOP_SEC)\n",
    "        for k, seg in enumerate(segs):\n",
    "            feats.append(mfcc_features(seg))\n",
    "            labs.append(y)\n",
    "            file_index.append({\"file\": str(p), \"segment\": int(k)})\n",
    "    X = np.stack(feats, axis=0)  # (Nseg, C, n_mfcc, T)\n",
    "    y = np.array(labs, dtype=np.int64)\n",
    "    return X, y, file_index\n",
    "\n",
    "print(\"Extrayendo TRAIN...\")\n",
    "X_train, y_train_seg, idx_train = process_labeled_file_list(paths_train, y_train)\n",
    "print(\"Extrayendo VAL...\")\n",
    "X_val, y_val_seg, idx_val = process_labeled_file_list(paths_val, y_val)\n",
    "print(\"Extrayendo TEST...\")\n",
    "X_test, y_test_seg, idx_test = process_labeled_file_list(paths_test, y_test)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"X_train:\", X_train.shape, \"y:\", y_train_seg.shape)\n",
    "print(\"X_val:\", X_val.shape, \"y:\", y_val_seg.shape)\n",
    "print(\"X_test:\", X_test.shape, \"y:\", y_test_seg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b9124c",
   "metadata": {},
   "source": [
    "## Celda 6 — Guardar ```.npy``` + metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21850f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta guardada en: C:\\Users\\leona\\Documents\\Thesis_Project_UACH\\Temp\\Dataset\\features_mfcc_labeled\\meta.json\n"
     ]
    }
   ],
   "source": [
    "# Para reducir memoria en disco:\n",
    "X_train = X_train.astype(np.float16)\n",
    "X_val   = X_val.astype(np.float16)\n",
    "X_test  = X_test.astype(np.float16)\n",
    "\n",
    "np.save(OUT_DIR / \"X_train.npy\", X_train)\n",
    "np.save(OUT_DIR / \"y_train.npy\", y_train_seg)\n",
    "np.save(OUT_DIR / \"X_val.npy\",   X_val)\n",
    "np.save(OUT_DIR / \"y_val.npy\",   y_val_seg)\n",
    "np.save(OUT_DIR / \"X_test.npy\",  X_test)\n",
    "np.save(OUT_DIR / \"y_test.npy\",  y_test_seg)\n",
    "\n",
    "with open(OUT_DIR / \"files_train.json\", \"w\") as f:\n",
    "    json.dump(idx_train, f, indent=2)\n",
    "with open(OUT_DIR / \"files_val.json\", \"w\") as f:\n",
    "    json.dump(idx_val, f, indent=2)\n",
    "with open(OUT_DIR / \"files_test.json\", \"w\") as f:\n",
    "    json.dump(idx_test, f, indent=2)\n",
    "\n",
    "class_meaning = {\n",
    "    0: \"original / con reina funcional\",\n",
    "    1: \"no presente\",\n",
    "    2: \"presente y rechazada\",\n",
    "    3: \"presente y recién aceptada\"\n",
    "}\n",
    "\n",
    "with open(OUT_DIR / \"meta.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"data_dir\": str(DATA_DIR),\n",
    "        \"csv_path\": str(CSV_PATH),\n",
    "        \"id_col\": ID_COL,\n",
    "        \"target_col\": TARGET_COL,\n",
    "        \"classes\": [0, 1, 2, 3],\n",
    "        \"class_meaning\": class_meaning,\n",
    "        \"sr\": SR,\n",
    "        \"trim_db\": TRIM_DB,\n",
    "        \"segment_seconds\": SEG_SEC,\n",
    "        \"hop_seconds\": HOP_SEC,\n",
    "        \"n_mfcc\": N_MFCC,\n",
    "        \"add_deltas\": ADD_DELTAS,\n",
    "        \"n_fft\": N_FFT,\n",
    "        \"hop_len\": HOP_LEN,\n",
    "        \"fmin\": FMIN,\n",
    "        \"fmax\": FMAX,\n",
    "        \"random_seed\": RANDOM_SEED\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Meta guardada en:\", OUT_DIR / \"meta.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
